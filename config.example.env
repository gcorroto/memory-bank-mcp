# Memory Bank MCP Server Configuration
# Copy this file to .env and fill in the values

# OpenAI API Key (required)
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# Memory Bank Storage Configuration
# Path where the vector database will be stored (relative to workspace)
MEMORYBANK_STORAGE_PATH=.memorybank

# Embedding Model Configuration
# Model to use for generating embeddings
MEMORYBANK_EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimensions (1536 for max accuracy, 512 for lower cost)
MEMORYBANK_EMBEDDING_DIMENSIONS=1536

# Chunking Configuration
# Maximum tokens per chunk (text-embedding-3-small limit is 8192)
# Default: 7500 tokens (with safety margin)
MEMORYBANK_MAX_TOKENS=7500

# Overlap between consecutive chunks in tokens to maintain context
# Default: 200 tokens
MEMORYBANK_CHUNK_OVERLAP_TOKENS=200

# Legacy options (deprecated, kept for backwards compatibility)
# MEMORYBANK_CHUNK_SIZE=1000
# MEMORYBANK_CHUNK_OVERLAP=200

# ===== Project Knowledge Layer Configuration =====
# These settings control the AI-powered documentation generation

# Reasoning model for generating project documentation
# Options: gpt-5, gpt-5-mini (recommended), gpt-5-nano
# Fallback: gpt-4o if Responses API is not available
MEMORYBANK_REASONING_MODEL=gpt-5-mini

# Reasoning effort level (affects cost and quality)
# Options: low (fast, cheaper), medium (balanced), high (thorough, expensive)
MEMORYBANK_REASONING_EFFORT=medium

# Auto-update project documentation after each indexing
# Set to true to keep docs in sync with code changes (uses reasoning API)
MEMORYBANK_AUTO_UPDATE_DOCS=false

# Optional: Workspace root path (defaults to current directory)
# MEMORYBANK_WORKSPACE_ROOT=/path/to/your/project

# Optional: MCP Server Configuration
MCP_SERVER_TYPE=stdio